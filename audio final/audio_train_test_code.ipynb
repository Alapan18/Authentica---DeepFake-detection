{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:25:11.725039Z",
     "iopub.status.busy": "2025-04-12T08:25:11.724090Z",
     "iopub.status.idle": "2025-04-12T08:25:25.661054Z",
     "shell.execute_reply": "2025-04-12T08:25:25.660379Z",
     "shell.execute_reply.started": "2025-04-12T08:25:11.725004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from joblib import dump, load\n",
    "\n",
    "# Use tqdm.auto for notebook/script compatibility\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, roc_auc_score,\n",
    "    precision_recall_fscore_support, confusion_matrix\n",
    ")\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Ignore librosa warnings about audioread potentially failing on MP3s\n",
    "warnings.filterwarnings('ignore', module='librosa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:25:25.662732Z",
     "iopub.status.busy": "2025-04-12T08:25:25.662198Z",
     "iopub.status.idle": "2025-04-12T08:25:25.668369Z",
     "shell.execute_reply": "2025-04-12T08:25:25.667563Z",
     "shell.execute_reply.started": "2025-04-12T08:25:25.662711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# IMPORTANT: Change this path to the root directory of your extracted dataset\n",
    "DATASET_PATH = 'C:/Users/USER/Downloads/Authentica---DeepFake-detection/audio_final_new/archive/for-2sec/for-2seconds/training'\n",
    "REAL_DIR = os.path.join(DATASET_PATH, 'real')\n",
    "FAKE_DIR = os.path.join(DATASET_PATH, 'fake')\n",
    "\n",
    "SAMPLE_RATE = 16000  # Sample rate for loading audio\n",
    "SEED = 42          # Random seed for reproducibility\n",
    "\n",
    "# Feature Extraction Params\n",
    "N_MFCC = 20         # Number of MFCCs\n",
    "\n",
    "# Spectrogram Params\n",
    "N_MELS = 128        # Number of Mel bands\n",
    "HOP_LENGTH = 512    # Hop length for STFT/Mel spectrogram\n",
    "N_FFT = 2048        # FFT window size\n",
    "SPEC_MAX_LEN = 63 # Fixed time dimension for spectrograms (ADJUST BASED ON YOUR DATA ANALYSIS)\n",
    "\n",
    "# Model Training Params\n",
    "TEST_SIZE = 0.2     # Proportion of data for validation\n",
    "BATCH_SIZE = 32     # Batch size for CNN training\n",
    "EPOCHS = 50         # Max epochs for CNN (EarlyStopping likely stops sooner)\n",
    "XGB_PARAMS = {      # Example XGBoost parameters (tune these)\n",
    "    'objective': 'binary:logistic',\n",
    "    'early_stopping_rounds': 20,\n",
    "    'eval_metric': 'logloss', # Use 'auc' or 'error' as well\n",
    "    'eta': 0.1,          # Learning rate\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.8,    # Fraction of samples used per tree\n",
    "    'colsample_bytree': 0.8, # Fraction of features used per tree\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0.1,        # Minimum loss reduction to make a split\n",
    "    'lambda': 1,         # L2 regularization\n",
    "    'alpha': 0,          # L1 regularization\n",
    "    'seed': SEED\n",
    "}\n",
    "CNN_LEARNING_RATE = 0.0001\n",
    "EARLY_STOPPING_PATIENCE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation\n",
    "\n",
    "Load audio file paths, assign labels (0=Real, 1=Fake), shuffle, and split into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:25:25.669376Z",
     "iopub.status.busy": "2025-04-12T08:25:25.669154Z",
     "iopub.status.idle": "2025-04-12T08:25:26.185953Z",
     "shell.execute_reply": "2025-04-12T08:25:26.185271Z",
     "shell.execute_reply.started": "2025-04-12T08:25:25.669359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Scanning dataset directories...\")\n",
    "filepaths = []\n",
    "labels = []\n",
    "\n",
    "# Ensure directories exist\n",
    "if not os.path.isdir(REAL_DIR):\n",
    "    raise ValueError(f\"Real directory not found: {REAL_DIR}\\nPlease check DATASET_PATH.\")\n",
    "if not os.path.isdir(FAKE_DIR):\n",
    "    raise ValueError(f\"Fake directory not found: {FAKE_DIR}\\nPlease check DATASET_PATH.\")\n",
    "\n",
    "# Load real files\n",
    "print(f\"Looking for audio files in: {REAL_DIR}\")\n",
    "real_files = [f for f in os.listdir(REAL_DIR) if f.lower().endswith(('.wav', '.mp3', '.flac'))]\n",
    "print(f\"Found {len(real_files)} potential real files.\")\n",
    "for filename in tqdm(real_files, desc=\"Loading real file paths\"):\n",
    "    filepaths.append(os.path.join(REAL_DIR, filename))\n",
    "    labels.append(0) # 0 for real\n",
    "\n",
    "# Load fake files\n",
    "print(f\"\\nLooking for audio files in: {FAKE_DIR}\")\n",
    "fake_files = [f for f in os.listdir(FAKE_DIR) if f.lower().endswith(('.wav', '.mp3', '.flac'))]\n",
    "print(f\"Found {len(fake_files)} potential fake files.\")\n",
    "for filename in tqdm(fake_files, desc=\"Loading fake file paths\"):\n",
    "    filepaths.append(os.path.join(FAKE_DIR, filename))\n",
    "    labels.append(1) # 1 for fake\n",
    "\n",
    "if not filepaths:\n",
    "    raise ValueError(f\"No audio files found in {REAL_DIR} or {FAKE_DIR}. Check paths and file extensions.\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'filepath': filepaths, 'label': labels})\n",
    "df = df.sample(frac=1, random_state=SEED).reset_index(drop=True) # Shuffle\n",
    "\n",
    "print(f\"\\nFound {len(df)} total audio files.\")\n",
    "print(\"Label distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Split data (using indices for consistency across models)\n",
    "train_indices, val_indices = train_test_split(\n",
    "    df.index,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED,\n",
    "    stratify=df['label'] # Ensure proportional splits\n",
    ")\n",
    "\n",
    "train_df = df.loc[train_indices].reset_index(drop=True)\n",
    "val_df = df.loc[val_indices].reset_index(drop=True)\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "y_val = val_df['label'].values\n",
    "\n",
    "print(f\"\\nTraining set size: {len(train_df)} ({len(y_train)})\")\n",
    "print(f\"Validation set size: {len(val_df)} ({len(y_val)})\")\n",
    "print(\"Training label distribution:\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(\"Validation label distribution:\")\n",
    "print(val_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "Functions for loading audio, extracting aggregated features, and generating spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:25:26.187865Z",
     "iopub.status.busy": "2025-04-12T08:25:26.187655Z",
     "iopub.status.idle": "2025-04-12T08:25:26.199513Z",
     "shell.execute_reply": "2025-04-12T08:25:26.198815Z",
     "shell.execute_reply.started": "2025-04-12T08:25:26.187849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_audio(filepath, sr=SAMPLE_RATE):\n",
    "    \"\"\"Loads an audio file using librosa, handling potential errors.\"\"\"\n",
    "    try:\n",
    "        # duration=None loads the entire file\n",
    "        audio, _ = librosa.load(filepath, sr=sr, duration=None, res_type='kaiser_fast')\n",
    "        if len(audio) == 0:\n",
    "            print(f\"Warning: Empty audio loaded from {filepath}\")\n",
    "            return None\n",
    "        return audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_features(audio, sr=SAMPLE_RATE, n_mfcc=N_MFCC):\n",
    "    \"\"\"Extracts aggregated audio features (mean and std dev).\"\"\"\n",
    "    if audio is None or len(audio) == 0:\n",
    "        # Return a zero vector of the expected size\n",
    "        # Size = (n_mfcc mean + n_mfcc std) + (chroma mean + std) + (spec_contrast mean + std) + (zcr mean + std) + (rms mean)\n",
    "        feature_size = n_mfcc * 2 + 2 + 2 + 2 + 1\n",
    "        return np.zeros(feature_size)\n",
    "\n",
    "    features = []\n",
    "    # MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "    features.extend(np.mean(mfccs, axis=1))\n",
    "    features.extend(np.std(mfccs, axis=1))\n",
    "\n",
    "    # Chroma Features\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "    features.append(np.mean(chroma))\n",
    "    features.append(np.std(chroma))\n",
    "\n",
    "    # Spectral Contrast\n",
    "    spec_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
    "    features.append(np.mean(spec_contrast))\n",
    "    features.append(np.std(spec_contrast))\n",
    "\n",
    "    # Zero-Crossing Rate\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=audio)\n",
    "    features.append(np.mean(zcr))\n",
    "    features.append(np.std(zcr))\n",
    "\n",
    "    # RMS Energy\n",
    "    rms = librosa.feature.rms(y=audio)\n",
    "    features.append(np.mean(rms))\n",
    "    # std(rms) is often very small and might not be informative\n",
    "    # features.append(np.std(rms))\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "def generate_spectrogram(audio, sr=SAMPLE_RATE, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, max_len=SPEC_MAX_LEN):\n",
    "    \"\"\"Generates a Mel Spectrogram and pads/truncates it to a fixed length.\"\"\"\n",
    "    if audio is None:\n",
    "        # Return an empty spectrogram of the target shape\n",
    "        return np.zeros((n_mels, max_len))\n",
    "\n",
    "    try:\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "        # Convert to decibels (log scale)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        # Pad or truncate the time dimension (axis=1)\n",
    "        current_len = mel_spec_db.shape[1]\n",
    "        if current_len < max_len:\n",
    "            pad_width = max_len - current_len\n",
    "            # Pad with the minimum value of the spectrogram (or a constant like -80 dB)\n",
    "            mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant', constant_values=np.min(mel_spec_db))\n",
    "        elif current_len > max_len:\n",
    "            mel_spec_db = mel_spec_db[:, :max_len]\n",
    "\n",
    "        return mel_spec_db\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating spectrogram: {e}\")\n",
    "        return np.zeros((n_mels, max_len))\n",
    "\n",
    "def normalize_spectrogram(spec):\n",
    "    \"\"\"Normalizes a single spectrogram to the range [0, 1].\"\"\"\n",
    "    min_val = np.min(spec)\n",
    "    max_val = np.max(spec)\n",
    "    if max_val > min_val:\n",
    "        # Normalize to [0, 1]\n",
    "        return (spec - min_val) / (max_val - min_val)\n",
    "    elif max_val == min_val:\n",
    "         # Handle constant spectrogram (e.g., silence)\n",
    "         return np.zeros_like(spec)\n",
    "    return spec # Should not happen if max_val > min_val check works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost Model Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:25:26.200889Z",
     "iopub.status.busy": "2025-04-12T08:25:26.200434Z",
     "iopub.status.idle": "2025-04-12T08:33:51.254174Z",
     "shell.execute_reply": "2025-04-12T08:33:51.253477Z",
     "shell.execute_reply.started": "2025-04-12T08:25:26.200863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Extracting features for XGBoost...\")\n",
    "X_train_features_list = []\n",
    "X_val_features_list = []\n",
    "\n",
    "# Process training data\n",
    "print(\"Processing Training Data:\")\n",
    "for filepath in tqdm(train_df['filepath'], desc=\"Extracting Train Features\"):\n",
    "    audio = load_audio(filepath, sr=SAMPLE_RATE)\n",
    "    features = extract_features(audio, sr=SAMPLE_RATE, n_mfcc=N_MFCC)\n",
    "    X_train_features_list.append(features)\n",
    "\n",
    "# Process validation data\n",
    "print(\"\\nProcessing Validation Data:\")\n",
    "for filepath in tqdm(val_df['filepath'], desc=\"Extracting Val Features\"):\n",
    "    audio = load_audio(filepath, sr=SAMPLE_RATE)\n",
    "    features = extract_features(audio, sr=SAMPLE_RATE, n_mfcc=N_MFCC)\n",
    "    X_val_features_list.append(features)\n",
    "\n",
    "X_train_features = np.array(X_train_features_list)\n",
    "X_val_features = np.array(X_val_features_list)\n",
    "\n",
    "# Handle potential NaNs or Infs resulting from silent audio or errors\n",
    "print(f\"\\nFeatures before NaN/Inf handling - Train: {np.isnan(X_train_features).sum()} NaNs, {np.isinf(X_train_features).sum()} Infs\")\n",
    "print(f\"Features before NaN/Inf handling - Val: {np.isnan(X_val_features).sum()} NaNs, {np.isinf(X_val_features).sum()} Infs\")\n",
    "X_train_features = np.nan_to_num(X_train_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_val_features = np.nan_to_num(X_val_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(f\"\\nFeature array shape (Train): {X_train_features.shape}\")\n",
    "print(f\"Feature array shape (Val): {X_val_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:33:51.255633Z",
     "iopub.status.busy": "2025-04-12T08:33:51.254946Z",
     "iopub.status.idle": "2025-04-12T08:33:51.280654Z",
     "shell.execute_reply": "2025-04-12T08:33:51.279927Z",
     "shell.execute_reply.started": "2025-04-12T08:33:51.255592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_features)\n",
    "X_val_scaled = scaler.transform(X_val_features)\n",
    "\n",
    "print(f\"Scaled feature shape (Train): {X_train_scaled.shape}\")\n",
    "print(f\"Scaled feature shape (Val): {X_val_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 XGBoost Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:33:51.281599Z",
     "iopub.status.busy": "2025-04-12T08:33:51.281394Z",
     "iopub.status.idle": "2025-04-12T08:33:51.777484Z",
     "shell.execute_reply": "2025-04-12T08:33:51.776942Z",
     "shell.execute_reply.started": "2025-04-12T08:33:51.281583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Training XGBoost Model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming XGB_PARAMS includes the necessary params for XGBClassifier\n",
    "xgb_model = xgb.XGBClassifier(**XGB_PARAMS)\n",
    "\n",
    "# Train with early stopping based on validation loss\n",
    "xgb_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    eval_set=[(X_val_scaled, y_val)],  # Validation set for early stopping\n",
    "    #early_stopping_rounds=EARLY_STOPPING_PATIENCE,  # Stop if validation loss doesn't improve\n",
    "    verbose=True  # Set to True for detailed logs\n",
    ")\n",
    "\n",
    "xgb_train_time = time.time() - start_time\n",
    "\n",
    "# Save the trained model to disk\n",
    "dump(xgb_model, 'C:/Users/USER/Downloads/Authentica---DeepFake-detection/audio_final_new/checkpoints_xgb/xgb_model.pkl')\n",
    "\n",
    "print(f\"XGBoost training completed in {xgb_train_time:.2f} seconds.\")\n",
    "print(f\"Best iteration: {xgb_model.best_iteration}\")\n",
    "print(f\"Best score ({XGB_PARAMS.get('eval_metric', 'logloss')}): {xgb_model.best_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 XGBoost Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:33:51.779097Z",
     "iopub.status.busy": "2025-04-12T08:33:51.778898Z",
     "iopub.status.idle": "2025-04-12T08:33:51.806544Z",
     "shell.execute_reply": "2025-04-12T08:33:51.805936Z",
     "shell.execute_reply.started": "2025-04-12T08:33:51.779081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"--- XGBoost Evaluation ---\")\n",
    "# Get probabilities for the positive class (Fake)\n",
    "xgb_pred_proba = xgb_model.predict_proba(X_val_scaled)[:, 1]\n",
    "# Get class predictions based on 0.5 threshold\n",
    "xgb_pred_labels = (xgb_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "xgb_accuracy = accuracy_score(y_val, xgb_pred_labels)\n",
    "xgb_precision, xgb_recall, xgb_f1, _ = precision_recall_fscore_support(y_val, xgb_pred_labels, average='binary', zero_division=0)\n",
    "try:\n",
    "    xgb_auc = roc_auc_score(y_val, xgb_pred_proba)\n",
    "except ValueError as e:\n",
    "    print(f\"AUC calculation warning: {e}. Probabilities might be non-finite or constant.\")\n",
    "    # Handle cases with potential issues (e.g., clipping)\n",
    "    xgb_auc = roc_auc_score(y_val, np.clip(xgb_pred_proba, 1e-7, 1 - 1e-7))\n",
    "\n",
    "print(f\"Accuracy: {xgb_accuracy:.4f}\")\n",
    "print(f\"Precision: {xgb_precision:.4f}\")\n",
    "print(f\"Recall: {xgb_recall:.4f}\")\n",
    "print(f\"F1-Score: {xgb_f1:.4f}\")\n",
    "print(f\"AUC: {xgb_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, xgb_pred_labels, target_names=['Real (0)', 'Fake (1)'], zero_division=0))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, xgb_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CNN Model Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Spectrogram Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:33:51.807584Z",
     "iopub.status.busy": "2025-04-12T08:33:51.807321Z",
     "iopub.status.idle": "2025-04-12T08:35:41.201199Z",
     "shell.execute_reply": "2025-04-12T08:35:41.200543Z",
     "shell.execute_reply.started": "2025-04-12T08:33:51.807560Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Generating Spectrograms for CNN...\")\n",
    "X_train_spec_list = []\n",
    "X_val_spec_list = []\n",
    "\n",
    "# Process training data\n",
    "print(\"Processing Training Data:\")\n",
    "for filepath in tqdm(train_df['filepath'], desc=\"Generating Train Spectrograms\"):\n",
    "    audio = load_audio(filepath, sr=SAMPLE_RATE)\n",
    "    spectrogram = generate_spectrogram(audio, sr=SAMPLE_RATE, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, max_len=SPEC_MAX_LEN)\n",
    "    X_train_spec_list.append(spectrogram)\n",
    "\n",
    "# Process validation data\n",
    "print(\"\\nProcessing Validation Data:\")\n",
    "for filepath in tqdm(val_df['filepath'], desc=\"Generating Val Spectrograms\"):\n",
    "    audio = load_audio(filepath, sr=SAMPLE_RATE)\n",
    "    spectrogram = generate_spectrogram(audio, sr=SAMPLE_RATE, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, max_len=SPEC_MAX_LEN)\n",
    "    X_val_spec_list.append(spectrogram)\n",
    "\n",
    "X_train_spec = np.array(X_train_spec_list)\n",
    "X_val_spec = np.array(X_val_spec_list)\n",
    "\n",
    "# Reshape for CNN (add channel dimension: height, width, channels)\n",
    "X_train_spec = X_train_spec[..., np.newaxis]\n",
    "X_val_spec = X_val_spec[..., np.newaxis]\n",
    "\n",
    "print(f\"\\nSpectrogram array shape (Train): {X_train_spec.shape}\")\n",
    "print(f\"Spectrogram array shape (Val): {X_val_spec.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Spectrogram Normalization\n",
    "\n",
    "Normalize spectrogram pixel values (dB scale) to the range [0, 1] for better CNN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:35:41.203305Z",
     "iopub.status.busy": "2025-04-12T08:35:41.203086Z",
     "iopub.status.idle": "2025-04-12T08:35:42.182708Z",
     "shell.execute_reply": "2025-04-12T08:35:42.181923Z",
     "shell.execute_reply.started": "2025-04-12T08:35:41.203288Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Normalizing spectrograms...\")\n",
    "\n",
    "# Apply normalization per spectrogram\n",
    "X_train_spec_norm = np.array([normalize_spectrogram(s) for s in tqdm(X_train_spec, desc=\"Normalizing Train Specs\")])\n",
    "X_val_spec_norm = np.array([normalize_spectrogram(s) for s in tqdm(X_val_spec, desc=\"Normalizing Val Specs\")])\n",
    "\n",
    "# Check min/max values after normalization (should be close to 0 and 1)\n",
    "print(f\"\\nTrain spec norm min/max: {np.min(X_train_spec_norm):.2f} / {np.max(X_train_spec_norm):.2f}\")\n",
    "print(f\"Val spec norm min/max: {np.min(X_val_spec_norm):.2f} / {np.max(X_val_spec_norm):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 CNN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:35:42.183937Z",
     "iopub.status.busy": "2025-04-12T08:35:42.183672Z",
     "iopub.status.idle": "2025-04-12T08:35:44.461275Z",
     "shell.execute_reply": "2025-04-12T08:35:44.460666Z",
     "shell.execute_reply.started": "2025-04-12T08:35:42.183909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_cnn_model(input_shape):\n",
    "    \"\"\"Defines the 2D CNN architecture.\"\"\"\n",
    "    model = Sequential(name=\"Spectrogram_CNN\")\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    # Layer 1\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flatten and Dense Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5)) # Dropout for regularization\n",
    "    model.add(Dense(1, activation='sigmoid')) # Output layer for binary classification\n",
    "\n",
    "    return model\n",
    "\n",
    "# Get the input shape from the normalized training data\n",
    "input_shape_cnn = X_train_spec_norm.shape[1:]\n",
    "print(f\"CNN Input Shape: {input_shape_cnn}\")\n",
    "\n",
    "cnn_model = build_cnn_model(input_shape_cnn)\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer=Adam(learning_rate=CNN_LEARNING_RATE),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 CNN Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:35:44.462315Z",
     "iopub.status.busy": "2025-04-12T08:35:44.462051Z",
     "iopub.status.idle": "2025-04-12T08:37:10.096478Z",
     "shell.execute_reply": "2025-04-12T08:37:10.095815Z",
     "shell.execute_reply.started": "2025-04-12T08:35:44.462295Z"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Training CNN Model...\")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',         # Metric to monitor\n",
    "    patience=EARLY_STOPPING_PATIENCE, # Number of epochs with no improvement\n",
    "    restore_best_weights=True, # Restore model weights from the epoch with the best monitored metric\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Optional: Save the best model checkpoint\n",
    "# model_checkpoint = ModelCheckpoint('best_cnn_model.keras', monitor='val_loss', save_best_only=True, verbose=0)\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath='C:/Users/USER/Downloads/Authentica---DeepFake-detection/audio_final_new/checkpoints_cnn/cnn_epoch_{epoch:02d}_{accuracy:02f}.h5',  # Save model after each epoch\n",
    "    save_freq='epoch',\n",
    "    save_weights_only=False,   # Set to True if you only want to save weights\n",
    "    save_best_only=False,      # Saves every epoch, not just the best one\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "history = cnn_model.fit(\n",
    "    X_train_spec_norm, y_train,\n",
    "    validation_data=(X_val_spec_norm, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint_cb], #early_stopping], # Add model_checkpoint here if using\n",
    "    verbose=1 # Set to 2 for less output per epoch, 0 for silent\n",
    ")\n",
    "cnn_train_time = time.time() - start_time\n",
    "print(f\"\\nCNN training completed in {cnn_train_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 CNN Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:37:10.098018Z",
     "iopub.status.busy": "2025-04-12T08:37:10.097354Z",
     "iopub.status.idle": "2025-04-12T08:37:11.872052Z",
     "shell.execute_reply": "2025-04-12T08:37:11.871376Z",
     "shell.execute_reply.started": "2025-04-12T08:37:10.097998Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"--- CNN Evaluation ---\")\n",
    "from tensorflow.keras.models import load_model\n",
    "cnn_model = load_model('C:/Users/USER/Downloads/Authentica---DeepFake-detection/audio_final_new/checkpoints_cnn/cnn_epoch_16_1.000000.h5') # Load the best model\n",
    "# Evaluate the model (using weights restored by EarlyStopping)\n",
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(X_val_spec_norm, y_val, verbose=0)\n",
    "\n",
    "# Get probabilities and labels\n",
    "cnn_pred_proba = cnn_model.predict(X_val_spec_norm).flatten() # Flatten to get shape (n_samples,)\n",
    "cnn_pred_labels = (cnn_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "cnn_precision, cnn_recall, cnn_f1, _ = precision_recall_fscore_support(y_val, cnn_pred_labels, average='binary', zero_division=0)\n",
    "try:\n",
    "    cnn_auc = roc_auc_score(y_val, cnn_pred_proba)\n",
    "except ValueError as e:\n",
    "    print(f\"AUC calculation warning: {e}. Probabilities might be non-finite or constant.\")\n",
    "    cnn_auc = roc_auc_score(y_val, np.clip(cnn_pred_proba, 1e-7, 1 - 1e-7))\n",
    "\n",
    "print(f\"Validation Loss: {cnn_loss:.4f}\")\n",
    "print(f\"Accuracy: {cnn_accuracy:.4f}\")\n",
    "print(f\"Precision: {cnn_precision:.4f}\")\n",
    "print(f\"Recall: {cnn_recall:.4f}\")\n",
    "print(f\"F1-Score: {cnn_f1:.4f}\")\n",
    "print(f\"AUC: {cnn_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, cnn_pred_labels, target_names=['Real (0)', 'Fake (1)'], zero_division=0))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, cnn_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Plot CNN Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:37:11.873036Z",
     "iopub.status.busy": "2025-04-12T08:37:11.872820Z",
     "iopub.status.idle": "2025-04-12T08:37:12.628137Z",
     "shell.execute_reply": "2025-04-12T08:37:12.627334Z",
     "shell.execute_reply.started": "2025-04-12T08:37:11.873020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if history:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('CNN Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title('CNN Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping history plots as CNN training did not run or complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ensemble Model\n",
    "\n",
    "Combine the predictions from XGBoost and the CNN using simple averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:37:12.630339Z",
     "iopub.status.busy": "2025-04-12T08:37:12.630085Z",
     "iopub.status.idle": "2025-04-12T08:37:12.650236Z",
     "shell.execute_reply": "2025-04-12T08:37:12.649665Z",
     "shell.execute_reply.started": "2025-04-12T08:37:12.630320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating Ensemble Model (Simple Averaging)...\")\n",
    "\n",
    "# --- Sanity Checks ---\n",
    "# Check if both models produced predictions\n",
    "if 'xgb_pred_proba' not in locals() or 'cnn_pred_proba' not in locals():\n",
    "     raise RuntimeError(\"One or both models did not produce predictions. Cannot create ensemble.\")\n",
    "\n",
    "# Check if prediction arrays have the same length as the validation set\n",
    "if len(xgb_pred_proba) != len(y_val):\n",
    "     raise ValueError(f\"XGBoost prediction length ({len(xgb_pred_proba)}) doesn't match validation labels ({len(y_val)}).\")\n",
    "if len(cnn_pred_proba) != len(y_val):\n",
    "     raise ValueError(f\"CNN prediction length ({len(cnn_pred_proba)}) doesn't match validation labels ({len(y_val)}).\")\n",
    "\n",
    "# --- Simple Averaging Ensemble ---\n",
    "ensemble_pred_proba = (xgb_pred_proba + cnn_pred_proba) / 2.0\n",
    "\n",
    "# Convert averaged probabilities to class labels\n",
    "ensemble_pred_labels = (ensemble_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# --- Evaluate Ensemble ---\n",
    "print(\"\\n--- Ensemble Evaluation ---\")\n",
    "ens_accuracy = accuracy_score(y_val, ensemble_pred_labels)\n",
    "ens_precision, ens_recall, ens_f1, _ = precision_recall_fscore_support(y_val, ensemble_pred_labels, average='binary', zero_division=0)\n",
    "\n",
    "try:\n",
    "    ens_auc = roc_auc_score(y_val, ensemble_pred_proba)\n",
    "except ValueError as e:\n",
    "    print(f\"AUC calculation warning: {e}. Clipping probabilities for calculation.\")\n",
    "    # Attempt calculation after clipping probabilities slightly away from 0 and 1\n",
    "    ens_auc = roc_auc_score(y_val, np.clip(ensemble_pred_proba, 1e-7, 1 - 1e-7))\n",
    "\n",
    "print(f\"Accuracy: {ens_accuracy:.4f}\")\n",
    "print(f\"Precision: {ens_precision:.4f}\")\n",
    "print(f\"Recall: {ens_recall:.4f}\")\n",
    "print(f\"F1-Score: {ens_f1:.4f}\")\n",
    "print(f\"AUC: {ens_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, ensemble_pred_labels, target_names=['Real (0)', 'Fake (1)'], zero_division=0))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, ensemble_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:37:12.651399Z",
     "iopub.status.busy": "2025-04-12T08:37:12.651008Z",
     "iopub.status.idle": "2025-04-12T08:37:12.656840Z",
     "shell.execute_reply": "2025-04-12T08:37:12.656107Z",
     "shell.execute_reply.started": "2025-04-12T08:37:12.651381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"--- Performance Comparison ---\")\n",
    "print(\"                                         \")\n",
    "print(f\"Metric      | XGBoost |   CNN   | Ensemble\")\n",
    "print(f\"------------|---------|---------|----------\")\n",
    "print(f\"Accuracy    | {xgb_accuracy:7.4f} | {cnn_accuracy:7.4f} | {ens_accuracy:8.4f}\")\n",
    "print(f\"Precision   | {xgb_precision:7.4f} | {cnn_precision:7.4f} | {ens_precision:8.4f}\")\n",
    "print(f\"Recall      | {xgb_recall:7.4f} | {cnn_recall:7.4f} | {ens_recall:8.4f}\")\n",
    "print(f\"F1-Score    | {xgb_f1:7.4f} | {cnn_f1:7.4f} | {ens_f1:8.4f}\")\n",
    "print(f\"AUC         | {xgb_auc:7.4f} | {cnn_auc:7.4f} | {ens_auc:8.4f}\")\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:40:41.298858Z",
     "iopub.status.busy": "2025-04-12T08:40:41.298101Z",
     "iopub.status.idle": "2025-04-12T08:42:13.216341Z",
     "shell.execute_reply": "2025-04-12T08:42:13.215735Z",
     "shell.execute_reply.started": "2025-04-12T08:40:41.298833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TEST_DATASET_PATH = 'C:/Users/USER/Downloads/Authentica---DeepFake-detection/audio_final_new/archive/for-2sec/for-2seconds/validation'\n",
    "TEST_REAL_DIR = os.path.join(TEST_DATASET_PATH, 'real')\n",
    "TEST_FAKE_DIR = os.path.join(TEST_DATASET_PATH, 'fake')\n",
    "\n",
    "# Check if the test directories exist\n",
    "if not os.path.isdir(TEST_DATASET_PATH):\n",
    "    print(f\"WARNING: Test dataset path not found: {TEST_DATASET_PATH}\")\n",
    "    print(\"Skipping evaluation on the separate test set.\")\n",
    "    run_test_evaluation = False\n",
    "else:\n",
    "    if not os.path.isdir(TEST_REAL_DIR):\n",
    "         raise ValueError(f\"Test 'real' directory not found: {TEST_REAL_DIR}\")\n",
    "    if not os.path.isdir(TEST_FAKE_DIR):\n",
    "         raise ValueError(f\"Test 'fake' directory not found: {TEST_FAKE_DIR}\")\n",
    "    run_test_evaluation = True\n",
    "\n",
    "# %% [code]\n",
    "# --- Load Test Data Filepaths and Labels ---\n",
    "if run_test_evaluation:\n",
    "    print(f\"Loading test data from: {TEST_DATASET_PATH}\")\n",
    "    test_filepaths = []\n",
    "    test_labels = []\n",
    "\n",
    "    # Load real files\n",
    "    print(f\"Looking for audio files in: {TEST_REAL_DIR}\")\n",
    "    test_real_files = [f for f in os.listdir(TEST_REAL_DIR) if f.lower().endswith(('.wav', '.mp3', '.flac'))]\n",
    "    print(f\"Found {len(test_real_files)} potential real test files.\")\n",
    "    for filename in tqdm(test_real_files, desc=\"Loading real test file paths\"):\n",
    "        test_filepaths.append(os.path.join(TEST_REAL_DIR, filename))\n",
    "        test_labels.append(0) # 0 for real\n",
    "\n",
    "    # Load fake files\n",
    "    print(f\"\\nLooking for audio files in: {TEST_FAKE_DIR}\")\n",
    "    test_fake_files = [f for f in os.listdir(TEST_FAKE_DIR) if f.lower().endswith(('.wav', '.mp3', '.flac'))]\n",
    "    print(f\"Found {len(test_fake_files)} potential fake test files.\")\n",
    "    for filename in tqdm(test_fake_files, desc=\"Loading fake test file paths\"):\n",
    "        test_filepaths.append(os.path.join(TEST_FAKE_DIR, filename))\n",
    "        test_labels.append(1) # 1 for fake\n",
    "\n",
    "    if not test_filepaths:\n",
    "        print(\"WARNING: No audio files found in the test dataset directories. Skipping evaluation.\")\n",
    "        run_test_evaluation = False\n",
    "    else:\n",
    "        # Create test DataFrame (optional, but good practice)\n",
    "        test_df = pd.DataFrame({'filepath': test_filepaths, 'label': test_labels})\n",
    "        # No shuffling needed for testing, but keep track of true labels\n",
    "        y_test = test_df['label'].values\n",
    "        print(f\"\\nLoaded {len(test_df)} test files.\")\n",
    "        print(\"Test set label distribution:\")\n",
    "        print(test_df['label'].value_counts())\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "# --- Preprocess Test Data for XGBoost ---\n",
    "if run_test_evaluation:\n",
    "    print(\"\\nPreprocessing test data for XGBoost...\")\n",
    "    X_test_features_list = []\n",
    "\n",
    "    # Extract features\n",
    "    for filepath in tqdm(test_df['filepath'], desc=\"Extracting Test Features\"):\n",
    "        audio = load_audio(filepath, sr=SAMPLE_RATE)\n",
    "        features = extract_features(audio, sr=SAMPLE_RATE, n_mfcc=N_MFCC)\n",
    "        X_test_features_list.append(features)\n",
    "\n",
    "    X_test_features = np.array(X_test_features_list)\n",
    "\n",
    "    # Handle NaNs/Infs\n",
    "    X_test_features = np.nan_to_num(X_test_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    print(f\"Test feature array shape: {X_test_features.shape}\")\n",
    "\n",
    "    # Scale features using the *original* scaler\n",
    "    print(\"Scaling test features using the scaler fitted on training data...\")\n",
    "    X_test_scaled = scaler.transform(X_test_features) # Use transform, NOT fit_transform\n",
    "    print(f\"Scaled test feature shape: {X_test_scaled.shape}\")\n",
    "\n",
    "# %% [code]\n",
    "# --- Preprocess Test Data for CNN ---\n",
    "if run_test_evaluation:\n",
    "    print(\"\\nPreprocessing test data for CNN...\")\n",
    "    X_test_spec_list = []\n",
    "\n",
    "    # Generate Spectrograms\n",
    "    for filepath in tqdm(test_df['filepath'], desc=\"Generating Test Spectrograms\"):\n",
    "        audio = load_audio(filepath, sr=SAMPLE_RATE)\n",
    "        # Use the SAME parameters as during training!\n",
    "        spectrogram = generate_spectrogram(audio, sr=SAMPLE_RATE, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, max_len=SPEC_MAX_LEN)\n",
    "        X_test_spec_list.append(spectrogram)\n",
    "\n",
    "    X_test_spec = np.array(X_test_spec_list)\n",
    "\n",
    "    # Reshape for CNN (add channel dimension)\n",
    "    X_test_spec = X_test_spec[..., np.newaxis]\n",
    "    print(f\"Test spectrogram array shape: {X_test_spec.shape}\")\n",
    "\n",
    "    # Normalize Spectrograms using the same method\n",
    "    print(\"Normalizing test spectrograms...\")\n",
    "    X_test_spec_norm = np.array([normalize_spectrogram(s) for s in tqdm(X_test_spec, desc=\"Normalizing Test Specs\")])\n",
    "    print(f\"Normalized test spectrogram shape: {X_test_spec_norm.shape}\")\n",
    "    print(f\"Test spec norm min/max: {np.min(X_test_spec_norm):.2f} / {np.max(X_test_spec_norm):.2f}\")\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "# --- Make Predictions on Test Data ---\n",
    "if run_test_evaluation:\n",
    "    print(\"\\nMaking predictions on the test set...\")\n",
    "\n",
    "    # XGBoost predictions\n",
    "    print(\"Predicting with XGBoost...\")\n",
    "    test_xgb_pred_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    print(f\"XGBoost test prediction probabilities shape: {test_xgb_pred_proba.shape}\")\n",
    "\n",
    "\n",
    "    # CNN predictions\n",
    "    print(\"Predicting with CNN...\")\n",
    "    test_cnn_pred_proba = cnn_model.predict(X_test_spec_norm, batch_size=BATCH_SIZE).flatten()\n",
    "    print(f\"CNN test prediction probabilities shape: {test_cnn_pred_proba.shape}\")\n",
    "\n",
    "    # Sanity check lengths\n",
    "    if len(test_xgb_pred_proba) != len(y_test) or len(test_cnn_pred_proba) != len(y_test):\n",
    "        raise ValueError(\"Prediction length mismatch with test labels!\")\n",
    "\n",
    "# %% [code]\n",
    "# --- Ensemble Predictions and Evaluation on Test Set ---\n",
    "if run_test_evaluation:\n",
    "    print(\"\\nEvaluating Ensemble Model on the Test Set...\")\n",
    "\n",
    "    # Simple Averaging Ensemble\n",
    "    test_ensemble_pred_proba = (test_xgb_pred_proba + test_cnn_pred_proba) / 2.0\n",
    "    test_ensemble_pred_labels = (test_ensemble_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # --- Evaluate Ensemble on Test Set ---\n",
    "    print(\"\\n--- Test Set Ensemble Evaluation ---\")\n",
    "    test_ens_accuracy = accuracy_score(y_test, test_ensemble_pred_labels)\n",
    "    test_ens_precision, test_ens_recall, test_ens_f1, _ = precision_recall_fscore_support(\n",
    "        y_test, test_ensemble_pred_labels, average='binary', zero_division=0\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        test_ens_auc = roc_auc_score(y_test, test_ensemble_pred_proba)\n",
    "    except ValueError as e:\n",
    "        print(f\"AUC calculation warning: {e}. Clipping probabilities for calculation.\")\n",
    "        test_ens_auc = roc_auc_score(y_test, np.clip(test_ensemble_pred_proba, 1e-7, 1 - 1e-7))\n",
    "\n",
    "    print(f\"Accuracy: {test_ens_accuracy:.4f}\")\n",
    "    print(f\"Precision: {test_ens_precision:.4f}\")\n",
    "    print(f\"Recall: {test_ens_recall:.4f}\")\n",
    "    print(f\"F1-Score: {test_ens_f1:.4f}\")\n",
    "    print(f\"AUC: {test_ens_auc:.4f}\")\n",
    "    print(\"\\nClassification Report (Test Set):\")\n",
    "    print(classification_report(y_test, test_ensemble_pred_labels, target_names=['Real (0)', 'Fake (1)'], zero_division=0))\n",
    "    print(\"Confusion Matrix (Test Set):\")\n",
    "    print(confusion_matrix(y_test, test_ensemble_pred_labels))\n",
    "\n",
    "# %% [code]\n",
    "# Optional: Evaluate individual models on the test set for comparison\n",
    "\n",
    "if run_test_evaluation:\n",
    "    print(\"\\n--- Individual Model Evaluation on Test Set ---\")\n",
    "\n",
    "    # XGBoost Only\n",
    "    test_xgb_pred_labels = (test_xgb_pred_proba > 0.5).astype(int)\n",
    "    xgb_test_acc = accuracy_score(y_test, test_xgb_pred_labels)\n",
    "    try:\n",
    "      xgb_test_auc = roc_auc_score(y_test, test_xgb_pred_proba)\n",
    "    except ValueError:\n",
    "       xgb_test_auc = roc_auc_score(y_test, np.clip(test_xgb_pred_proba, 1e-7, 1-1e-7))\n",
    "    # CNN Only\n",
    "    test_cnn_pred_labels = (test_cnn_pred_proba > 0.5).astype(int)\n",
    "    cnn_test_acc = accuracy_score(y_test, test_cnn_pred_labels)\n",
    "    try:\n",
    "        cnn_test_auc = roc_auc_score(y_test, test_cnn_pred_proba)\n",
    "    except ValueError:\n",
    "       cnn_test_auc = roc_auc_score(y_test, np.clip(test_cnn_pred_proba, 1e-7, 1-1e-7))\n",
    "    print(f\"Ensemble Test Accuracy: {test_ens_accuracy:.4f}, AUC: {test_ens_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
