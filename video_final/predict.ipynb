{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import face_recognition\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import random\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNeXt50_32X4D_Weights\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames are  [375]\n"
     ]
    }
   ],
   "source": [
    "path_to_videos = ['C:/Users/USER/Downloads/Authentica---DeepFake-detection/testcases/WhatsApp Video 2025-04-11 at 13.29.25_7e6ebfc1.mp4']\n",
    "\n",
    "#path_to_videos = ['D:/btech/research/app_copy/app_copy/video/full fake.mp4']\n",
    "\n",
    "frame_count = []\n",
    "for video_file in path_to_videos:\n",
    "  cap = cv2.VideoCapture(video_file)\n",
    "  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "print(\"frames are \" , frame_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes,latent_dim= 2048, lstm_layers=1 , hidden_dim = 2048, bidirectional = False):\n",
    "        super(Model, self).__init__()\n",
    "        model = models.resnext50_32x4d(weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1)\n",
    "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
    "        self.lstm = nn.LSTM(latent_dim,hidden_dim, lstm_layers,  bidirectional)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dp = nn.Dropout(0.4)\n",
    "        self.linear1 = nn.Linear(2048,num_classes)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    def forward(self, x):\n",
    "        batch_size,seq_length, c, h, w = x.shape\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        fmap = self.model(x)\n",
    "        x = self.avgpool(fmap)\n",
    "        x = x.view(batch_size,seq_length,2048)\n",
    "        x_lstm,_ = self.lstm(x,None)\n",
    "        return fmap,self.dp(self.linear1(x_lstm[:,-1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 112\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "sm = nn.Softmax(dim=1)\n",
    "inv_normalize =  transforms.Normalize(mean=-1*np.divide(mean,std),std=np.divide([1,1,1],std))\n",
    "def im_convert(tensor):\n",
    "    \"\"\" Display a tensor as an image. \"\"\"\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.squeeze()\n",
    "    image = inv_normalize(image)\n",
    "    image = image.numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image.clip(0, 1)\n",
    "    cv2.imwrite('./2.png',image*255)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize lists to store predictions and confidence values for each frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img, path='./'):\n",
    "    # Ensure the model is in evaluation mode and the input is on the correct device\n",
    "    model.eval()\n",
    "\n",
    "    if not isinstance(img, torch.Tensor) or img.shape[0] == 0:\n",
    "        print(\"Warning: Empty or invalid input image tensor.\")\n",
    "        return None\n",
    "\n",
    "    # Ensure the input is on the correct device\n",
    "    img = img.to('cuda')\n",
    "\n",
    "    # Loop through each frame in the batch\n",
    "    for i in range(img.size(1)):  # img.size(1) gives the number of frames\n",
    "        frame = img[:, i, :, :, :].unsqueeze(0)  # Extract each frame and add batch dimension\n",
    "\n",
    "        # Forward pass\n",
    "        fmap, logits = model(frame)\n",
    "\n",
    "        # Extract weights from the final linear layer\n",
    "        try:\n",
    "            weight_softmax = model.linear1.weight.detach().cpu().numpy()\n",
    "        except AttributeError:\n",
    "            print(\"Error: Model does not have a layer named 'linear1'.\")\n",
    "            return None\n",
    "\n",
    "        # Softmax and prediction\n",
    "        logits = torch.nn.functional.softmax(logits, dim=1)  # Ensure softmax is applied\n",
    "        _, prediction = torch.max(logits, 1)\n",
    "        confidence = logits[:, int(prediction.item())].item() * 100\n",
    "\n",
    "        # Store results\n",
    "        if int(prediction.item()) == 1:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "        confidences.append(confidence)\n",
    "\n",
    "        #print(f'Frame {i + 1}: Prediction = {prediction.item()}, Confidence = {confidence:.2f}%')\n",
    "\n",
    "    # Optionally, return the list of predictions and confidences\n",
    "    return [predictions, confidences]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new implementation\n",
    "\n",
    "class validation_dataset(Dataset):\n",
    "    def __init__(self,video_names,sequence_length = 60,transform = None):\n",
    "        self.video_names = video_names\n",
    "        self.transform = transform\n",
    "        self.count = sequence_length\n",
    "    def __len__(self):\n",
    "        return len(self.video_names)\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_names[idx]\n",
    "        frames = []\n",
    "\n",
    "        # Extract all frames from the video\n",
    "        for frame in self.frame_extract(video_path):\n",
    "            frames.append(self.transform(frame))\n",
    "        #print(\"frames: \",frames,\"\\n Length of frames: \",len(frames))\n",
    "        # Stack all frames into a tensor\n",
    "        frames = torch.stack(frames)\n",
    "        #print(\"frames: \",frames,\"\\n Length of frames: \",len(frames))\n",
    "        # Add an extra dimension (batch dimension)\n",
    "        #print(\"frames: \",frames,\"\\n Length of frames: \",len(frames))\n",
    "        return frames.unsqueeze(0)\n",
    "\n",
    "\n",
    "    def frame_extract(self,path):\n",
    "      vidObj = cv2.VideoCapture(path) \n",
    "      success = 1\n",
    "      while success:\n",
    "          success, image = vidObj.read()\n",
    "          if success:\n",
    "              yield image\n",
    "              \n",
    "def im_plot(tensor):\n",
    "    image = tensor.cpu().numpy().transpose(1, 2, 0)\n",
    "    b, g, r = cv2.split(image)\n",
    "    image = cv2.merge((r, g, b))\n",
    "    image = image * [0.22803, 0.22145, 0.216989] + [0.43216, 0.394666, 0.37645]\n",
    "    image = image * 255.0\n",
    "    plt.imshow(image.astype(int))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "  transforms.ToPILImage(),\n",
    "  transforms.Resize((im_size,im_size)),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(mean,std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "video_dataset = validation_dataset(path_to_videos,sequence_length = 20,transform = train_transforms)\n",
    "\n",
    "\n",
    "#print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(len(predictions))\\nprint(len(confidences))\\nfor i in range(0,len(predictions)):\\n    print(predictions[i], confidences[i] )'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(len(predictions))\n",
    "print(len(confidences))\n",
    "for i in range(0,len(predictions)):\n",
    "    print(predictions[i], confidences[i] )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_model_71.26_epoch_1.pt', 'train_model_76.19_epoch_2.pt', 'train_model_79.03_epoch_3.pt', 'train_model_81.47_epoch_4.pt', 'train_model_84.24_epoch_5.pt', 'train_model_85.42_epoch_6.pt', 'train_model_87.32_epoch_7.pt', 'train_model_88.43_epoch_8.pt', 'train_model_90.19_epoch_9.pt', 'train_model_91.52_epoch_10.pt', 'train_model_92.00_epoch_11.pt', 'train_model_92.95_epoch_12.pt', 'train_model_93.47_epoch_13.pt', 'train_model_93.68_epoch_14.pt', 'train_model_94.83_epoch_15.pt', 'train_model_94.93_epoch_16.pt', 'train_model_95.32_epoch_17.pt', 'train_model_95.74_epoch_19.pt', 'train_model_95.78_epoch_18.pt', 'train_model_96.12_epoch_20.pt', 'train_model_96.48_epoch_21.pt', 'train_model_96.64_epoch_23.pt', 'train_model_96.88_epoch_22.pt', 'train_model_97.17_epoch_24.pt', 'train_model_97.36_epoch_25.pt', 'train_model_97.37_epoch_28.pt', 'train_model_97.40_epoch_26.pt', 'train_model_97.44_epoch_27.pt', 'train_model_97.63_epoch_29.pt', 'train_model_97.65_epoch_43.pt', 'train_model_97.84_epoch_31.pt', 'train_model_97.88_epoch_32.pt', 'train_model_98.04_epoch_35.pt', 'train_model_98.09_epoch_33.pt', 'train_model_98.17_epoch_30.pt', 'train_model_98.19_epoch_44.pt', 'train_model_98.20_epoch_39.pt', 'train_model_98.33_epoch_37.pt', 'train_model_98.35_epoch_38.pt', 'train_model_98.36_epoch_34.pt', 'train_model_98.43_epoch_45.pt', 'train_model_98.53_epoch_41.pt', 'train_model_98.54_epoch_36.pt', 'train_model_98.54_epoch_46.pt', 'train_model_98.55_epoch_51.pt', 'train_model_98.66_epoch_40.pt', 'train_model_98.66_epoch_47.pt', 'train_model_98.69_epoch_49.pt', 'train_model_98.71_epoch_48.pt', 'train_model_98.74_epoch_42.pt', 'train_model_98.79_epoch_50.pt', 'train_model_98.82_epoch_52.pt', 'train_model_98.91_epoch_56.pt', 'train_model_98.97_epoch_57.pt', 'train_model_99.00_epoch_74.pt', 'train_model_99.02_epoch_53.pt', 'train_model_99.02_epoch_63.pt', 'train_model_99.04_epoch_65.pt', 'train_model_99.05_epoch_58.pt', 'train_model_99.07_epoch_61.pt', 'train_model_99.07_epoch_67.pt', 'train_model_99.10_epoch_55.pt', 'train_model_99.11_epoch_66.pt', 'train_model_99.12_epoch_54.pt', 'train_model_99.12_epoch_62.pt', 'train_model_99.12_epoch_72.pt', 'train_model_99.13_epoch_70.pt', 'train_model_99.19_epoch_59.pt', 'train_model_99.20_epoch_64.pt', 'train_model_99.24_epoch_73.pt', 'train_model_99.24_epoch_80.pt', 'train_model_99.24_epoch_83.pt', 'train_model_99.25_epoch_77.pt', 'train_model_99.27_epoch_75.pt', 'train_model_99.27_epoch_81.pt', 'train_model_99.29_epoch_68.pt', 'train_model_99.29_epoch_90.pt', 'train_model_99.31_epoch_71.pt', 'train_model_99.34_epoch_60.pt', 'train_model_99.34_epoch_78.pt', 'train_model_99.34_epoch_79.pt', 'train_model_99.34_epoch_82.pt', 'train_model_99.35_epoch_92.pt', 'train_model_99.36_epoch_76.pt', 'train_model_99.37_epoch_69.pt', 'train_model_99.41_epoch_100.pt', 'train_model_99.41_epoch_94.pt', 'train_model_99.42_epoch_91.pt', 'train_model_99.43_epoch_85.pt', 'train_model_99.45_epoch_87.pt', 'train_model_99.49_epoch_88.pt', 'train_model_99.49_epoch_97.pt', 'train_model_99.50_epoch_86.pt', 'train_model_99.50_epoch_96.pt', 'train_model_99.51_epoch_89.pt', 'train_model_99.52_epoch_98.pt', 'train_model_99.52_epoch_99.pt', 'train_model_99.59_epoch_101.pt', 'train_model_99.59_epoch_95.pt', 'train_model_99.65_epoch_102.pt', 'train_model_99.67_epoch_84.pt', 'train_model_99.69_epoch_93.pt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to your folder\n",
    "folder_path = 'C:/Users/USER/Downloads/Authentica---DeepFake-detection/models'\n",
    "\n",
    "# List to store filenames\n",
    "model_names = []\n",
    "\n",
    "# Loop through items in folder\n",
    "for item in os.listdir(folder_path):\n",
    "    if os.path.isfile(os.path.join(folder_path, item)):\n",
    "        model_names.append(item)\n",
    "\n",
    "print(model_names)\n",
    "len(model_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mModel: train_model_71.26_epoch_1.pt | Fake | Confidence: 70.19%\u001b[0m\n",
      "\u001b[32mModel: train_model_76.19_epoch_2.pt | Real | Confidence: 64.95%\u001b[0m\n",
      "\u001b[32mModel: train_model_79.03_epoch_3.pt | Real | Confidence: 59.41%\u001b[0m\n",
      "\u001b[32mModel: train_model_81.47_epoch_4.pt | Real | Confidence: 57.72%\u001b[0m\n",
      "\u001b[31mModel: train_model_84.24_epoch_5.pt | Fake | Confidence: 56.80%\u001b[0m\n",
      "\u001b[31mModel: train_model_85.42_epoch_6.pt | Fake | Confidence: 65.40%\u001b[0m\n",
      "\u001b[31mModel: train_model_87.32_epoch_7.pt | Fake | Confidence: 54.58%\u001b[0m\n",
      "\u001b[31mModel: train_model_88.43_epoch_8.pt | Fake | Confidence: 64.83%\u001b[0m\n",
      "\u001b[32mModel: train_model_90.19_epoch_9.pt | Real | Confidence: 52.69%\u001b[0m\n",
      "\u001b[32mModel: train_model_91.52_epoch_10.pt | Real | Confidence: 51.16%\u001b[0m\n",
      "\u001b[31mModel: train_model_92.00_epoch_11.pt | Fake | Confidence: 58.91%\u001b[0m\n",
      "\u001b[31mModel: train_model_92.95_epoch_12.pt | Fake | Confidence: 54.93%\u001b[0m\n",
      "\u001b[31mModel: train_model_93.47_epoch_13.pt | Fake | Confidence: 53.56%\u001b[0m\n",
      "\u001b[31mModel: train_model_93.68_epoch_14.pt | Fake | Confidence: 65.36%\u001b[0m\n",
      "\u001b[32mModel: train_model_94.83_epoch_15.pt | Real | Confidence: 54.86%\u001b[0m\n",
      "\u001b[31mModel: train_model_94.93_epoch_16.pt | Fake | Confidence: 73.89%\u001b[0m\n",
      "\u001b[31mModel: train_model_95.32_epoch_17.pt | Fake | Confidence: 61.94%\u001b[0m\n",
      "\u001b[31mModel: train_model_95.74_epoch_19.pt | Fake | Confidence: 63.78%\u001b[0m\n",
      "\u001b[31mModel: train_model_95.78_epoch_18.pt | Fake | Confidence: 65.70%\u001b[0m\n",
      "\u001b[31mModel: train_model_96.12_epoch_20.pt | Fake | Confidence: 73.46%\u001b[0m\n",
      "\u001b[31mModel: train_model_96.48_epoch_21.pt | Fake | Confidence: 53.78%\u001b[0m\n",
      "\u001b[32mModel: train_model_96.64_epoch_23.pt | Real | Confidence: 50.45%\u001b[0m\n",
      "\u001b[31mModel: train_model_96.88_epoch_22.pt | Fake | Confidence: 69.08%\u001b[0m\n",
      "\u001b[31mModel: train_model_97.17_epoch_24.pt | Fake | Confidence: 62.74%\u001b[0m\n",
      "\u001b[31mModel: train_model_97.36_epoch_25.pt | Fake | Confidence: 68.56%\u001b[0m\n",
      "\u001b[31mModel: train_model_97.37_epoch_28.pt | Fake | Confidence: 74.99%\u001b[0m\n",
      "\u001b[31mModel: train_model_97.40_epoch_26.pt | Fake | Confidence: 60.55%\u001b[0m\n",
      "\u001b[31mModel: train_model_97.44_epoch_27.pt | Fake | Confidence: 63.44%\u001b[0m\n",
      "\u001b[31mModel: train_model_97.63_epoch_29.pt | Fake | Confidence: 64.14%\u001b[0m\n",
      "\u001b[31mModel: train_model_97.65_epoch_43.pt | Fake | Confidence: 59.70%\u001b[0m\n",
      "\u001b[31mModel: train_model_97.84_epoch_31.pt | Fake | Confidence: 77.82%\u001b[0m\n",
      "\u001b[31mModel: train_model_97.88_epoch_32.pt | Fake | Confidence: 58.81%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.04_epoch_35.pt | Fake | Confidence: 64.55%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.09_epoch_33.pt | Fake | Confidence: 62.64%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.17_epoch_30.pt | Fake | Confidence: 71.31%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.19_epoch_44.pt | Fake | Confidence: 56.32%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.20_epoch_39.pt | Fake | Confidence: 63.08%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.33_epoch_37.pt | Fake | Confidence: 64.65%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.35_epoch_38.pt | Fake | Confidence: 67.61%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.36_epoch_34.pt | Fake | Confidence: 65.89%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.43_epoch_45.pt | Fake | Confidence: 55.12%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.53_epoch_41.pt | Fake | Confidence: 50.16%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.54_epoch_36.pt | Fake | Confidence: 62.65%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.54_epoch_46.pt | Fake | Confidence: 61.77%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.55_epoch_51.pt | Fake | Confidence: 70.35%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.66_epoch_40.pt | Fake | Confidence: 67.50%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.66_epoch_47.pt | Fake | Confidence: 62.11%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.69_epoch_49.pt | Fake | Confidence: 61.40%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.71_epoch_48.pt | Fake | Confidence: 68.68%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.74_epoch_42.pt | Fake | Confidence: 57.79%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.79_epoch_50.pt | Fake | Confidence: 73.72%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.82_epoch_52.pt | Fake | Confidence: 64.84%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.91_epoch_56.pt | Fake | Confidence: 72.16%\u001b[0m\n",
      "\u001b[31mModel: train_model_98.97_epoch_57.pt | Fake | Confidence: 68.32%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.00_epoch_74.pt | Fake | Confidence: 64.34%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.02_epoch_53.pt | Fake | Confidence: 73.76%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.02_epoch_63.pt | Fake | Confidence: 70.74%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.04_epoch_65.pt | Fake | Confidence: 55.44%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.05_epoch_58.pt | Fake | Confidence: 62.96%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.07_epoch_61.pt | Fake | Confidence: 60.70%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.07_epoch_67.pt | Fake | Confidence: 55.81%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.10_epoch_55.pt | Fake | Confidence: 64.75%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.11_epoch_66.pt | Fake | Confidence: 54.96%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.12_epoch_54.pt | Fake | Confidence: 51.96%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.12_epoch_62.pt | Fake | Confidence: 61.94%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.12_epoch_72.pt | Fake | Confidence: 66.92%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.13_epoch_70.pt | Fake | Confidence: 60.30%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.19_epoch_59.pt | Fake | Confidence: 63.60%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.20_epoch_64.pt | Fake | Confidence: 71.48%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.24_epoch_73.pt | Fake | Confidence: 75.82%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.24_epoch_80.pt | Fake | Confidence: 61.59%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.24_epoch_83.pt | Fake | Confidence: 70.22%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.25_epoch_77.pt | Fake | Confidence: 69.45%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.27_epoch_75.pt | Fake | Confidence: 85.34%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.27_epoch_81.pt | Fake | Confidence: 66.82%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.29_epoch_68.pt | Fake | Confidence: 66.70%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.29_epoch_90.pt | Fake | Confidence: 63.44%\u001b[0m\n",
      "\u001b[32mModel: train_model_99.31_epoch_71.pt | Real | Confidence: 50.28%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.34_epoch_60.pt | Fake | Confidence: 66.37%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.34_epoch_78.pt | Fake | Confidence: 75.76%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.34_epoch_79.pt | Fake | Confidence: 69.88%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.34_epoch_82.pt | Fake | Confidence: 70.47%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.35_epoch_92.pt | Fake | Confidence: 78.56%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.36_epoch_76.pt | Fake | Confidence: 71.12%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.37_epoch_69.pt | Fake | Confidence: 65.68%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.41_epoch_100.pt | Fake | Confidence: 70.29%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.41_epoch_94.pt | Fake | Confidence: 68.42%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.42_epoch_91.pt | Fake | Confidence: 76.33%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.43_epoch_85.pt | Fake | Confidence: 65.97%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.45_epoch_87.pt | Fake | Confidence: 76.42%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.49_epoch_88.pt | Fake | Confidence: 72.33%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.49_epoch_97.pt | Fake | Confidence: 67.11%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.50_epoch_86.pt | Fake | Confidence: 73.28%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.50_epoch_96.pt | Fake | Confidence: 65.01%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.51_epoch_89.pt | Fake | Confidence: 89.21%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.52_epoch_98.pt | Fake | Confidence: 57.60%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.52_epoch_99.pt | Fake | Confidence: 71.85%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.59_epoch_101.pt | Fake | Confidence: 74.79%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.59_epoch_95.pt | Fake | Confidence: 60.96%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.65_epoch_102.pt | Fake | Confidence: 75.01%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.67_epoch_84.pt | Fake | Confidence: 66.93%\u001b[0m\n",
      "\u001b[31mModel: train_model_99.69_epoch_93.pt | Fake | Confidence: 71.65%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for z in model_names:\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    path_to_model = 'C:/Users/USER/Downloads/Authentica---DeepFake-detection/models/'+z\n",
    "    model = Model(2).cuda()\n",
    "    model.load_state_dict(torch.load(path_to_model))\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    # Initialize sum to accumulate confidence values\n",
    "    sum_confidence = 0\n",
    "\n",
    "    # Process the single video\n",
    "    p = predict(model, video_dataset[0], './')  # Assuming there's only one video in video_dataset\n",
    "\n",
    "    if p is not None:\n",
    "        predictions = p[0]\n",
    "        confidences = p[1]\n",
    "\n",
    "        # Accumulate confidence values based on predictions\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] == 1:  # Assuming 1 means 'real'\n",
    "                sum_confidence += confidences[i]\n",
    "            else:  # Assuming 0 means 'fake'\n",
    "                sum_confidence += 100 - confidences[i]\n",
    "\n",
    "        # Calculate the average confidence\n",
    "        avg_confidence = sum_confidence / len(predictions)\n",
    "\n",
    "        # Determine the final prediction for the video\n",
    "        if avg_confidence < 50 :\n",
    "            fake_confidence = 100 - avg_confidence\n",
    "            #if fake_confidence > 95:\n",
    "            print(f\"\\033[31mModel: {z} | Fake | Confidence: {fake_confidence:.2f}%\\033[0m\")\n",
    "        else:\n",
    "            #if avg_confidence > 95:\n",
    "            print(f\"\\033[32mModel: {z} | Real | Confidence: {avg_confidence:.2f}%\\033[0m\")\n",
    "    else:\n",
    "        print(\"Prediction failed for the video.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: train_model_71.26_epoch_1.pt | Fake | Confidence: 53.13%\n",
    "Model: train_model_76.19_epoch_2.pt | Fake | Confidence: 62.07%\n",
    "Model: train_model_79.03_epoch_3.pt | Fake | Confidence: 68.66%\n",
    "Model: train_model_81.47_epoch_4.pt | Fake | Confidence: 78.57%\n",
    "Model: train_model_84.24_epoch_5.pt | Fake | Confidence: 87.49%\n",
    "Model: train_model_85.42_epoch_6.pt | Fake | Confidence: 72.83%\n",
    "Model: train_model_87.32_epoch_7.pt | Fake | Confidence: 83.60%\n",
    "Model: train_model_88.43_epoch_8.pt | Fake | Confidence: 87.58%\n",
    "Model: train_model_90.19_epoch_9.pt | Fake | Confidence: 76.70%\n",
    "Model: train_model_91.52_epoch_10.pt | Fake | Confidence: 77.86%\n",
    "Model: train_model_92.00_epoch_11.pt | Fake | Confidence: 85.41%\n",
    "Model: train_model_92.95_epoch_12.pt | Fake | Confidence: 86.32%\n",
    "Model: train_model_93.47_epoch_13.pt | Fake | Confidence: 83.02%\n",
    "Model: train_model_93.68_epoch_14.pt | Fake | Confidence: 89.67%\n",
    "Model: train_model_94.83_epoch_15.pt | Fake | Confidence: 73.44%\n",
    "Model: train_model_94.93_epoch_16.pt | Fake | Confidence: 90.21%\n",
    "Model: train_model_95.32_epoch_17.pt | Fake | Confidence: 90.42%\n",
    "Model: train_model_95.74_epoch_19.pt | Fake | Confidence: 92.21%\n",
    "Model: train_model_95.78_epoch_18.pt | Fake | Confidence: 89.82%\n",
    "Model: train_model_96.12_epoch_20.pt | Fake | Confidence: 93.86%\n",
    "Model: train_model_96.48_epoch_21.pt | Fake | Confidence: 91.51%\n",
    "Model: train_model_96.64_epoch_23.pt | Fake | Confidence: 77.44%\n",
    "Model: train_model_96.88_epoch_22.pt | Fake | Confidence: 95.64%\n",
    "Model: train_model_97.17_epoch_24.pt | Fake | Confidence: 90.44%\n",
    "Model: train_model_97.36_epoch_25.pt | Fake | Confidence: 95.71%\n",
    "Model: train_model_97.37_epoch_28.pt | Fake | Confidence: 95.39%\n",
    "Model: train_model_97.40_epoch_26.pt | Fake | Confidence: 92.83%\n",
    "Model: train_model_97.44_epoch_27.pt | Fake | Confidence: 95.36%\n",
    "Model: train_model_97.63_epoch_29.pt | Fake | Confidence: 91.32%\n",
    "Model: train_model_97.65_epoch_43.pt | Fake | Confidence: 80.70%\n",
    "Model: train_model_97.84_epoch_31.pt | Fake | Confidence: 94.93%\n",
    "Model: train_model_97.88_epoch_32.pt | Fake | Confidence: 86.35%\n",
    "Model: train_model_98.04_epoch_35.pt | Fake | Confidence: 89.12%\n",
    "Model: train_model_98.09_epoch_33.pt | Fake | Confidence: 85.34%\n",
    "Model: train_model_98.17_epoch_30.pt | Fake | Confidence: 96.26%\n",
    "Model: train_model_98.19_epoch_44.pt | Fake | Confidence: 80.15%\n",
    "Model: train_model_98.20_epoch_39.pt | Fake | Confidence: 85.95%\n",
    "Model: train_model_98.33_epoch_37.pt | Fake | Confidence: 89.73%\n",
    "Model: train_model_98.35_epoch_38.pt | Fake | Confidence: 92.34%\n",
    "Model: train_model_98.36_epoch_34.pt | Fake | Confidence: 89.97%\n",
    "Model: train_model_98.43_epoch_45.pt | Fake | Confidence: 86.34%\n",
    "Model: train_model_98.53_epoch_41.pt | Fake | Confidence: 75.68%\n",
    "Model: train_model_98.54_epoch_36.pt | Fake | Confidence: 85.30%\n",
    "Model: train_model_98.54_epoch_46.pt | Fake | Confidence: 88.55%\n",
    "Model: train_model_98.55_epoch_51.pt | Fake | Confidence: 92.93%\n",
    "Model: train_model_98.66_epoch_40.pt | Fake | Confidence: 90.11%\n",
    "Model: train_model_98.66_epoch_47.pt | Fake | Confidence: 91.01%\n",
    "Model: train_model_98.69_epoch_49.pt | Fake | Confidence: 88.96%\n",
    "Model: train_model_98.71_epoch_48.pt | Fake | Confidence: 92.88%\n",
    "Model: train_model_98.74_epoch_42.pt | Fake | Confidence: 88.82%\n",
    "Model: train_model_98.79_epoch_50.pt | Fake | Confidence: 93.46%\n",
    "Model: train_model_98.82_epoch_52.pt | Fake | Confidence: 93.11%\n",
    "Model: train_model_98.91_epoch_56.pt | Fake | Confidence: 90.28%\n",
    "Model: train_model_98.97_epoch_57.pt | Fake | Confidence: 90.03%\n",
    "Model: train_model_99.00_epoch_74.pt | Fake | Confidence: 87.37%\n",
    "Model: train_model_99.02_epoch_53.pt | Fake | Confidence: 93.76%\n",
    "Model: train_model_99.02_epoch_63.pt | Fake | Confidence: 82.42%\n",
    "Model: train_model_99.04_epoch_65.pt | Fake | Confidence: 81.96%\n",
    "Model: train_model_99.05_epoch_58.pt | Fake | Confidence: 86.74%\n",
    "Model: train_model_99.07_epoch_61.pt | Fake | Confidence: 84.99%\n",
    "Model: train_model_99.07_epoch_67.pt | Fake | Confidence: 88.46%\n",
    "Model: train_model_99.10_epoch_55.pt | Fake | Confidence: 88.82%\n",
    "Model: train_model_99.11_epoch_66.pt | Fake | Confidence: 86.10%\n",
    "Model: train_model_99.12_epoch_54.pt | Fake | Confidence: 89.15%\n",
    "Model: train_model_99.12_epoch_62.pt | Fake | Confidence: 91.57%\n",
    "Model: train_model_99.12_epoch_72.pt | Fake | Confidence: 95.89%\n",
    "Model: train_model_99.13_epoch_70.pt | Fake | Confidence: 90.24%\n",
    "Model: train_model_99.19_epoch_59.pt | Fake | Confidence: 90.53%\n",
    "Model: train_model_99.20_epoch_64.pt | Fake | Confidence: 88.06%\n",
    "Model: train_model_99.24_epoch_73.pt | Fake | Confidence: 91.96%\n",
    "Model: train_model_99.24_epoch_80.pt | Fake | Confidence: 91.51%\n",
    "Model: train_model_99.24_epoch_83.pt | Fake | Confidence: 95.05%\n",
    "Model: train_model_99.25_epoch_77.pt | Fake | Confidence: 86.94%\n",
    "Model: train_model_99.27_epoch_75.pt | Fake | Confidence: 96.68%\n",
    "Model: train_model_99.27_epoch_81.pt | Fake | Confidence: 94.75%\n",
    "Model: train_model_99.29_epoch_68.pt | Fake | Confidence: 95.60%\n",
    "Model: train_model_99.29_epoch_90.pt | Fake | Confidence: 90.08%\n",
    "Model: train_model_99.31_epoch_71.pt | Fake | Confidence: 85.22%\n",
    "Model: train_model_99.34_epoch_60.pt | Fake | Confidence: 91.94%\n",
    "Model: train_model_99.34_epoch_78.pt | Fake | Confidence: 92.28%\n",
    "Model: train_model_99.34_epoch_79.pt | Fake | Confidence: 90.67%\n",
    "Model: train_model_99.34_epoch_82.pt | Fake | Confidence: 91.39%\n",
    "Model: train_model_99.35_epoch_92.pt | Fake | Confidence: 96.06%\n",
    "Model: train_model_99.36_epoch_76.pt | Fake | Confidence: 95.01%\n",
    "Model: train_model_99.37_epoch_69.pt | Fake | Confidence: 92.58%\n",
    "Model: train_model_99.41_epoch_100.pt | Fake | Confidence: 94.84%\n",
    "Model: train_model_99.41_epoch_94.pt | Fake | Confidence: 96.30%\n",
    "Model: train_model_99.42_epoch_91.pt | Fake | Confidence: 96.72%\n",
    "Model: train_model_99.43_epoch_85.pt | Fake | Confidence: 95.23%\n",
    "Model: train_model_99.45_epoch_87.pt | Fake | Confidence: 96.00%\n",
    "Model: train_model_99.49_epoch_88.pt | Fake | Confidence: 95.76%\n",
    "Model: train_model_99.49_epoch_97.pt | Fake | Confidence: 90.45%\n",
    "Model: train_model_99.50_epoch_86.pt | Fake | Confidence: 96.16%\n",
    "Model: train_model_99.50_epoch_96.pt | Fake | Confidence: 95.84%\n",
    "Model: train_model_99.51_epoch_89.pt | Fake | Confidence: 98.15%\n",
    "Model: train_model_99.52_epoch_98.pt | Fake | Confidence: 88.16%\n",
    "Model: train_model_99.52_epoch_99.pt | Fake | Confidence: 97.75%\n",
    "Model: train_model_99.59_epoch_101.pt | Fake | Confidence: 96.71%\n",
    "Model: train_model_99.59_epoch_95.pt | Fake | Confidence: 96.12%\n",
    "Model: train_model_99.65_epoch_102.pt | Fake | Confidence: 96.82%\n",
    "Model: train_model_99.67_epoch_84.pt | Fake | Confidence: 95.44%\n",
    "Model: train_model_99.69_epoch_93.pt | Fake | Confidence: 96.59%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Convert to TorchScript\\nscripted_model = torch.jit.script(model)\\nscripted_model.save(\"C:/Users/alapa/Documents/Deepfake_detection_using_deep_learning-master/Deepfake_detection_using_deep_learning-master/model_scripted.pt\")\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Convert to TorchScript\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save(\"C:/Users/alapa/Documents/Deepfake_detection_using_deep_learning-master/Deepfake_detection_using_deep_learning-master/model_scripted.pt\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
